<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>RLM Memory Retention Analysis - Executive Summary</title>
    <style>
        @page {
            size: letter;
            margin: 0.75in;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;
            font-size: 11pt;
            line-height: 1.5;
            color: #1a1a1a;
            background: white;
        }

        .page {
            page-break-after: always;
            min-height: 9in;
        }

        .page:last-child {
            page-break-after: avoid;
        }

        header {
            border-bottom: 3px solid #b17d1b;
            padding-bottom: 12px;
            margin-bottom: 20px;
        }

        h1 {
            font-size: 22pt;
            color: #b17d1b;
            margin-bottom: 4px;
            font-weight: 600;
        }

        .subtitle {
            font-size: 12pt;
            color: #555;
        }

        .meta {
            font-size: 9pt;
            color: #777;
            margin-top: 8px;
        }

        h2 {
            font-size: 14pt;
            color: #b17d1b;
            margin: 20px 0 10px;
            padding-bottom: 4px;
            border-bottom: 1px solid #d4a853;
        }

        h3 {
            font-size: 11pt;
            color: #333;
            margin: 14px 0 6px;
        }

        p {
            margin-bottom: 10px;
        }

        .highlight-box {
            background: #fef9f0;
            border: 1px solid #d4a853;
            border-left: 4px solid #b17d1b;
            padding: 12px 16px;
            margin: 16px 0;
            border-radius: 0 6px 6px 0;
        }

        .highlight-box.critical {
            background: #fef2f2;
            border-color: #dc2626;
            border-left-color: #dc2626;
        }

        .highlight-box.success {
            background: #f0fdf4;
            border-color: #16a34a;
            border-left-color: #16a34a;
        }

        .highlight-box h3 {
            margin-top: 0;
            color: inherit;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 12px 0;
            font-size: 10pt;
        }

        th, td {
            padding: 8px 10px;
            text-align: left;
            border: 1px solid #ddd;
        }

        th {
            background: #f5f0e6;
            color: #333;
            font-weight: 600;
        }

        tr:nth-child(even) {
            background: #fafafa;
        }

        .badge {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 10px;
            font-size: 9pt;
            font-weight: 600;
        }

        .badge.pass {
            background: #dcfce7;
            color: #166534;
        }

        .badge.fail {
            background: #fee2e2;
            color: #991b1b;
        }

        .two-col {
            display: flex;
            gap: 20px;
        }

        .two-col > div {
            flex: 1;
        }

        .metric-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 12px;
            margin: 12px 0;
        }

        .metric-card {
            background: #f8f8f8;
            border: 1px solid #e0e0e0;
            border-radius: 6px;
            padding: 12px;
            text-align: center;
        }

        .metric-card .value {
            font-size: 20pt;
            font-weight: 700;
            color: #b17d1b;
        }

        .metric-card .label {
            font-size: 9pt;
            color: #666;
            margin-top: 2px;
        }

        .metric-card.alert .value {
            color: #dc2626;
        }

        .metric-card.success .value {
            color: #16a34a;
        }

        ul {
            margin: 8px 0 8px 20px;
        }

        li {
            margin: 4px 0;
        }

        .quote-box {
            background: #f5f5f5;
            border-left: 3px solid #999;
            padding: 10px 14px;
            margin: 12px 0;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 9pt;
            color: #555;
        }

        .comparison-row {
            display: flex;
            gap: 12px;
            margin: 12px 0;
        }

        .comparison-card {
            flex: 1;
            border: 1px solid #ddd;
            border-radius: 6px;
            padding: 10px;
        }

        .comparison-card h4 {
            font-size: 10pt;
            margin-bottom: 6px;
            padding-bottom: 4px;
            border-bottom: 1px solid #eee;
        }

        .comparison-card.fail {
            border-color: #fca5a5;
            background: #fef2f2;
        }

        .comparison-card.fail h4 {
            color: #dc2626;
        }

        .comparison-card.pass {
            border-color: #86efac;
            background: #f0fdf4;
        }

        .comparison-card.pass h4 {
            color: #16a34a;
        }

        .footer {
            margin-top: 30px;
            padding-top: 12px;
            border-top: 1px solid #ddd;
            font-size: 9pt;
            color: #777;
            text-align: center;
        }

        @media print {
            body {
                -webkit-print-color-adjust: exact !important;
                print-color-adjust: exact !important;
            }
        }
    </style>
</head>
<body>
    <!-- PAGE 1 -->
    <div class="page">
        <header>
            <h1>RLM Memory Retention Analysis</h1>
            <p class="subtitle">Why Direct Chat Fails in Long Conversations and How RLM Solves It</p>
            <p class="meta">northstar.LM Performance Testing | January 19, 2026 | Test Run 20260119-4</p>
        </header>

        <div class="highlight-box critical">
            <h3>Critical Finding</h3>
            <p><strong>Direct Chat loses memory after ~15 conversation turns.</strong> In our 25-question stress test, Direct Chat explicitly failed to recall earlier responses, stating "I don't have response #X in the meeting data available here." RLM modes maintained 100% recall throughout.</p>
        </div>

        <div class="metric-grid">
            <div class="metric-card alert">
                <div class="value">5</div>
                <div class="label">Direct Chat Memory Failures</div>
            </div>
            <div class="metric-card success">
                <div class="value">0</div>
                <div class="label">RLM Memory Failures</div>
            </div>
            <div class="metric-card">
                <div class="value">25</div>
                <div class="label">Total Test Prompts</div>
            </div>
        </div>

        <h2>The Problem: Context Window Limitations</h2>
        <p>Direct Chat maintains all conversation history in a single context window. As conversations grow longer, earlier content gets compressed or effectively "forgotten" by the model. This manifests as:</p>
        <ul>
            <li>Inability to recall specific details from early responses</li>
            <li>Hedged answers like "based on what I can see here..."</li>
            <li>Explicit admissions: "I don't have that response available"</li>
        </ul>

        <h2>Evidence: Side-by-Side Response Comparison</h2>
        <p><strong>Test Question (Prompt 24):</strong> "Going back to your timeline in response #15 - what was the earliest date?"</p>

        <div class="comparison-row">
            <div class="comparison-card fail">
                <h4>Direct Chat <span class="badge fail">FAILED</span></h4>
                <div class="quote-box">"I don't have your response #15 timeline text in the meeting data I can see here, so I can't verify what you and I listed there."</div>
            </div>
            <div class="comparison-card pass">
                <h4>RLM+SWM <span class="badge pass">PASSED</span></h4>
                <div class="quote-box">"Earliest date in the timeline (response #15): Jan 30â€“31, 2024 - Event: FOMC Meeting 1, policy rate held at 5.25-5.50%"</div>
            </div>
        </div>

        <h2>How RLM Solves Memory Retention</h2>
        <div class="two-col">
            <div>
                <h3>Direct Chat Approach</h3>
                <ul>
                    <li>Single context window holds everything</li>
                    <li>Earlier content pushed out as conversation grows</li>
                    <li>No re-reading of source material</li>
                    <li>Memory degrades over time</li>
                </ul>
            </div>
            <div>
                <h3>RLM Approach</h3>
                <ul>
                    <li>Re-queries source agents each turn</li>
                    <li>Fresh context reconstruction every time</li>
                    <li>Conversation history in persistent memory store</li>
                    <li>No degradation regardless of length</li>
                </ul>
            </div>
        </div>

        <div class="highlight-box success">
            <h3>Key Insight</h3>
            <p>RLM doesn't need to "remember" earlier responses because it <strong>re-reads the source material</strong> for each query. This architectural difference makes it immune to context window limitations.</p>
        </div>
    </div>

    <!-- PAGE 2 -->
    <div class="page">
        <h2>Quantitative Results</h2>

        <table>
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Direct Chat</th>
                    <th>RLM+SWM</th>
                    <th>RLM+Hybrid</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Total Tokens</strong></td>
                    <td>566,966</td>
                    <td>895,181</td>
                    <td>1,138,561</td>
                </tr>
                <tr>
                    <td><strong>Total Cost</strong></td>
                    <td>$1.2102</td>
                    <td><strong>$1.0881</strong></td>
                    <td>$1.1083</td>
                </tr>
                <tr>
                    <td><strong>Avg Response Time</strong></td>
                    <td><strong>13.2s</strong></td>
                    <td>46.9s</td>
                    <td>47.0s</td>
                </tr>
                <tr>
                    <td><strong>Memory Recall Rate</strong></td>
                    <td style="color: #dc2626;">80%</td>
                    <td style="color: #16a34a;"><strong>100%</strong></td>
                    <td style="color: #16a34a;"><strong>100%</strong></td>
                </tr>
                <tr>
                    <td><strong>Memory Failures</strong></td>
                    <td style="color: #dc2626;">5 of 25</td>
                    <td style="color: #16a34a;">0 of 25</td>
                    <td style="color: #16a34a;">0 of 25</td>
                </tr>
                <tr>
                    <td><strong>Source Attribution</strong></td>
                    <td>None</td>
                    <td>Full</td>
                    <td>Full</td>
                </tr>
            </tbody>
        </table>

        <div class="highlight-box">
            <h3>Cost Paradox</h3>
            <p>Despite using <strong>58% more tokens</strong>, RLM+SWM costs <strong>10% less</strong> than Direct Chat. This is due to model tiering: GPT-5-mini handles sub-queries at ~1/10th the cost of GPT-5.2, while the final synthesis uses the full model.</p>
        </div>

        <h2>Quality Scores (5-Point Scale)</h2>
        <table>
            <thead>
                <tr>
                    <th>Criterion</th>
                    <th>Direct Chat</th>
                    <th>RLM+SWM</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Completeness</td>
                    <td>3.2</td>
                    <td><strong>4.6</strong></td>
                </tr>
                <tr>
                    <td>Accuracy (recall of prior responses)</td>
                    <td style="color: #dc2626;">2.8</td>
                    <td style="color: #16a34a;"><strong>4.8</strong></td>
                </tr>
                <tr>
                    <td>Coherence with conversation history</td>
                    <td>3.5</td>
                    <td><strong>4.7</strong></td>
                </tr>
                <tr>
                    <td>Source attribution</td>
                    <td>3.0</td>
                    <td><strong>4.9</strong></td>
                </tr>
                <tr>
                    <td>Consistency (no degradation)</td>
                    <td style="color: #dc2626;">2.0</td>
                    <td style="color: #16a34a;"><strong>4.8</strong></td>
                </tr>
                <tr style="background: #f5f0e6;">
                    <td><strong>Overall Average</strong></td>
                    <td><strong style="color: #dc2626;">2.9 / 5</strong></td>
                    <td><strong style="color: #16a34a;">4.76 / 5</strong></td>
                </tr>
            </tbody>
        </table>

        <h2>Recommendations</h2>
        <div class="two-col">
            <div>
                <h3>Use RLM+SWM When:</h3>
                <ul>
                    <li>Conversations will exceed 10-15 turns</li>
                    <li>Cross-referencing earlier responses is needed</li>
                    <li>Source attribution is important</li>
                    <li>Analytical accuracy matters more than speed</li>
                </ul>
            </div>
            <div>
                <h3>Use Direct Chat When:</h3>
                <ul>
                    <li>Quick, single-turn queries</li>
                    <li>Short conversations (&lt;10 turns)</li>
                    <li>Speed is critical</li>
                    <li>Simple fact retrieval</li>
                </ul>
            </div>
        </div>

        <h2>Next Steps</h2>
        <ol>
            <li><strong>Default to RLM+SWM</strong> for multi-agent analytical workflows</li>
            <li><strong>Add UI indicator</strong> warning users when Direct Chat exceeds 15 turns</li>
            <li><strong>Disable Shadow/Focus by default</strong> (adds 27% token overhead with no benefit in fresh sessions)</li>
            <li><strong>Adjust Auto-RLM thresholds</strong> to switch earlier in conversations</li>
        </ol>

        <div class="footer">
            <p>northstar.LM Memory Retention Analysis | Test Run 20260119-4 | Generated January 19, 2026</p>
            <p>Full technical report: Memory-Degradation-Analysis-Report.html</p>
        </div>
    </div>
</body>
</html>
