# RLM Implementation Status

> **Last Updated:** January 13, 2026 (Phase 3 Complete)  
> **Based on:** "Recursive Language Models" by Zhang, Kraska & Khattab ([arXiv:2512.24601](https://arxiv.org/abs/2512.24601))

---

## Executive Summary

The northstar.LM Agent Orchestrator now includes a full **RLM** implementation with **Phase 1: REPL Environment**, **Phase 2: True Recursion**, and **Phase 3: Optimization & Caching** complete. This enables intelligent query decomposition, parallel execution, response aggregation, in-browser Python code execution via Pyodide, **synchronous recursive LLM calls from within Python code**, and **LRU-based query result caching for improved performance**.

| Component | Status | Notes |
|-----------|--------|-------|
| Query Decomposition | âœ… Complete | 4 strategies: direct, parallel, map-reduce, iterative |
| Sub-Query Execution | âœ… Complete | Parallel execution with concurrency control |
| Response Aggregation | âœ… Complete | LLM synthesis with source attribution |
| REPL Environment | âœ… Complete | Pyodide-based Python execution in Web Worker |
| Code Generator | âœ… Complete | Query classification, retry logic, few-shot examples |
| True Recursion | âœ… Complete | `sub_lm()` calls with SharedArrayBuffer sync, depth limit 3 |
| Async Fallback | âœ… Complete | Graceful degradation without SharedArrayBuffer |
| Train of Thought UI | âœ… Complete | Enhanced UI with step logging and progress callbacks |
| Unified Service Worker | âœ… Complete | COI + PWA merged into `sw.js` v4, no reload loops |
| Progress Callbacks | âœ… Complete | Real-time updates from RLM pipeline to UI |
| Query Result Cache | âœ… Complete | LRU cache with TTL, auto-invalidation on agent changes |
| sub_lm Progress UI | âœ… Complete | Real-time progress for recursive calls with depth indicators |
| Token Optimization | âœ… Complete | Compact context methods, token budget management |

---

## Phase 1: REPL Environment Foundation âœ… COMPLETE

### What Was Implemented

#### 1. REPL Worker (`js/rlm/repl-worker.js`)
- Web Worker running Pyodide v0.25.0
- Sandboxed Python execution environment
- Message-based communication with main thread
- Built-in Python API functions:
  - `partition(predicate)` - Filter agents by condition
  - `grep(pattern, field)` - Search agent content
  - `search_agents(query)` - Semantic search
  - `get_agent(id)` - Retrieve single agent
  - `list_agents()` - List all agents
  - `get_all_action_items()` - Aggregate action items
  - `get_all_summaries()` - Aggregate summaries
  - `FINAL(answer)` - Mark final answer
  - `FINAL_VAR(name)` - Return variable as final answer

#### 2. REPL Environment (`js/rlm/repl-environment.js`)
- Main interface for REPL operations
- Lazy initialization (Pyodide loads ~10MB on first use)
- Context synchronization with ContextStore
- Execution with configurable timeout
- Ready state management

#### 3. Code Generator (`js/rlm/code-generator.js`)
- LLM prompt generation for Python code
- Code validation and sanitization
- Output parsing for `FINAL()` and `FINAL_VAR()` tags
- Error handling and fallback logic

#### 4. Context Store Updates (`js/rlm/context-store.js`)
- `toPythonDict()` method for REPL integration
- Agent data export at multiple detail levels
- Relevance scoring for query-agent matching

#### 5. RLM Pipeline Integration (`js/rlm/index.js`)
- `initializeREPL()` - Lazy REPL initialization
- `processWithREPL()` - Code-assisted query handling
- `shouldUseREPL()` - Query routing logic
- REPL stats tracking

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Agent Orchestrator                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Query     â”‚     â”‚   Context   â”‚     â”‚    Code     â”‚       â”‚
â”‚  â”‚ Decomposer  â”‚     â”‚    Store    â”‚     â”‚  Generator  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚               RLM Pipeline (index.js)                â”‚       â”‚
â”‚  â”‚                                                       â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚
â”‚  â”‚  â”‚Decomposeâ”‚â†’ â”‚ Execute â”‚â†’ â”‚Aggregateâ”‚â†’ â”‚ Format  â”‚  â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚
â”‚  â”‚       â”‚                                      â–²        â”‚       â”‚
â”‚  â”‚       â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚       â”‚
â”‚  â”‚       â–¼   â–¼                                           â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚
â”‚  â”‚  â”‚            REPL Environment                      â”‚  â”‚       â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚       â”‚
â”‚  â”‚  â”‚  â”‚         Web Worker (Pyodide)            â”‚    â”‚  â”‚       â”‚
â”‚  â”‚  â”‚  â”‚                                          â”‚    â”‚  â”‚       â”‚
â”‚  â”‚  â”‚  â”‚  agents_data = [...]  # From Context    â”‚    â”‚  â”‚       â”‚
â”‚  â”‚  â”‚  â”‚  result = analyze(agents_data)           â”‚    â”‚  â”‚       â”‚
â”‚  â”‚  â”‚  â”‚  FINAL(result)                           â”‚    â”‚  â”‚       â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Files Created/Modified

| File | Type | Description |
|------|------|-------------|
| `js/rlm/repl-worker.js` | New | Pyodide Web Worker |
| `js/rlm/repl-environment.js` | New | REPL interface class |
| `js/rlm/code-generator.js` | New | Code generation & parsing |
| `js/rlm/context-store.js` | Modified | Added `toPythonDict()` |
| `js/rlm/index.js` | Modified | REPL integration in pipeline |
| `orchestrator.html` | Modified | Pyodide preconnect, About section |
| `CLAUDE.md` | Modified | Documentation updates |

---

## Current Capabilities

### Query Routing

The RLM pipeline automatically routes queries based on complexity:

| Condition | Route | Strategy |
|-----------|-------|----------|
| â‰¤2 active agents, simple query | Legacy | Direct LLM call |
| 3+ agents, comparative query | RLM | Parallel decomposition |
| "all", "every", "across" keywords | RLM | Map-reduce |
| Code/calculation keywords | REPL | Python execution |
| Complex multi-part query | RLM | Iterative with follow-up |

### Decomposition Strategies

```javascript
// Direct - Simple queries, few agents
{ type: 'direct', subQueries: 1 }

// Parallel - Compare across agents
{ type: 'parallel', subQueries: N (one per relevant agent) }

// Map-Reduce - Aggregate across all
{ type: 'map-reduce', subQueries: N + 1 (gather + synthesize) }

// Iterative - Exploratory with uncertainty
{ type: 'iterative', subQueries: 2-3 (initial + follow-ups) }
```

### REPL Triggers

Queries containing these patterns trigger REPL-based processing:

- `"calculate"`, `"compute"`, `"count"`
- `"find all"`, `"list all"`, `"filter"`
- `"aggregate"`, `"sum"`, `"average"`
- `"python"`, `"code"`, `"script"`

---

## Phase 2: True Recursion âœ… COMPLETE

### What Was Implemented

#### Phase 2.1: Enhanced LLM Code Generation

**File: `js/rlm/code-generator.js`**

1. **Query Classification** - Automatic detection of query type:
   - `FACTUAL` - Simple fact-finding queries
   - `AGGREGATIVE` - Queries that gather data across meetings
   - `COMPARATIVE` - Queries comparing multiple meetings
   - `SEARCH` - Queries looking for specific content
   - `RECURSIVE` - Complex analysis requiring `sub_lm()`

2. **Retry Logic** - `generateWithRetry()` regenerates code on validation failure with error context

3. **Enhanced Examples** - Few-shot examples for each query type, including recursive patterns

#### Phase 2.2: True Recursion with `sub_lm()`

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Main Thread                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚               REPL Environment                           â”‚    â”‚
â”‚  â”‚  - SharedArrayBuffer (64KB)                              â”‚    â”‚
â”‚  â”‚  - LLM callback handler                                  â”‚    â”‚
â”‚  â”‚  - Atomics.notify() to signal worker                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â–²                                     â”‚
â”‚                            â”‚ SUB_LM request/response             â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚               REPL Worker (Pyodide)                      â”‚    â”‚
â”‚  â”‚  - Python sub_lm() function                              â”‚    â”‚
â”‚  â”‚  - Atomics.wait() for blocking                           â”‚    â”‚
â”‚  â”‚  - Depth tracking (MAX_DEPTH = 3)                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**

1. **SharedArrayBuffer Sync** (`js/rlm/repl-worker.js`)
   - 64KB shared buffer for LLM responses
   - `Atomics.wait()` for synchronous blocking in worker
   - Response encoding/decoding via TextEncoder

2. **SUB_LM Handler** (`js/rlm/repl-environment.js`)
   - Receives `SUB_LM` messages from worker
   - Calls LLM via registered callback
   - Writes response to shared buffer
   - Signals worker via `Atomics.notify()`

3. **Python Integration** (`js/rlm/repl-worker.js`)
   ```python
   def sub_lm(query, context_slice=None):
       """Synchronous recursive LLM call."""
       if _current_depth >= MAX_DEPTH:
           raise RecursionError("Max depth exceeded")
       _current_depth += 1
       try:
           return _sub_lm_sync(query, context_slice)
       finally:
           _current_depth -= 1
   ```

4. **Pipeline Integration** (`js/rlm/index.js`)
   - `processWithREPL()` sets LLM callback before execution
   - Query classification for optimal code generation
   - Sub-LM stats tracking

### Browser Requirements

SharedArrayBuffer requires:
- HTTPS or localhost
- `Cross-Origin-Opener-Policy: same-origin`
- `Cross-Origin-Embedder-Policy: require-corp`

**GitHub Pages Solution:** COI headers are now injected by the unified service worker (`sw.js`). The service worker adds COOP/COEP headers to all responses, enabling SharedArrayBuffer on static hosts like GitHub Pages.

**Fallback:** If SharedArrayBuffer is unavailable, `sub_lm()` returns placeholders that are processed after code execution completes.

### Files Modified

| File | Changes |
|------|---------|
| `js/rlm/code-generator.js` | Query classification, retry logic, recursive examples |
| `js/rlm/repl-worker.js` | SharedArrayBuffer sync, Python `sub_lm()` with depth tracking |
| `js/rlm/repl-environment.js` | SUB_LM handler, LLM callback, sync buffer management |
| `js/rlm/index.js` | Pipeline integration, progress callbacks, stats, exports |

---

## Phase 2.3: Enhanced Train of Thought & Service Worker Unification âœ… COMPLETE

### What Was Implemented

#### 1. Enhanced Train of Thought UI

The chat now displays a detailed, real-time train of thought during query processing:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ¤–  RLM: Code-Assisted Analysis             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â†’ Query: "What decisions were made?"        â”‚
â”‚ ðŸ·ï¸ Mode: REPL with 3 agents                 â”‚
â”‚ ðŸ·ï¸ Query type: AGGREGATIVE (95% confidence) â”‚
â”‚ ðŸ Calling GPT to generate Python code      â”‚
â”‚ âœ“ Python code generated (1 attempt)         â”‚
â”‚ âš¡ Executing Python in Pyodide sandbox       â”‚
â”‚ âœ“ Python code executed successfully          â”‚
â”‚ ðŸ“Š Extracting FINAL answer from output       â”‚
â”‚ âœ“ Response ready                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â³ Formatting...                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step Types with Icons:**
| Type | Icon | Color | Description |
|------|------|-------|-------------|
| `classify` | ðŸ·ï¸ | Purple | Query classification |
| `decompose` | ðŸ”€ | Blue | Query decomposition |
| `code` | ðŸ | Green | Code generation |
| `execute` | âš¡ | Amber | Execution |
| `recurse` | ðŸ”„ | Pink | Recursive LLM calls |
| `aggregate` | ðŸ“Š | Cyan | Result aggregation |
| `success` | âœ“ | Bright green | Completion |
| `warning` | âš ï¸ | Orange | Warnings/fallbacks |

#### 2. Progress Callbacks from RLM Pipeline

The RLM pipeline now emits real-time progress updates:

```javascript
// Set progress callback
rlmPipeline.setProgressCallback((step, type, details) => {
    addThinkingStep(thinkingId, step, type);
});

// Pipeline emits progress at key stages:
// - Query classification
// - Code generation start/complete
// - Execution start/complete
// - Sub-LM calls
// - Aggregation
```

#### 3. Unified Service Worker

The COI and PWA service workers were merged into a single `sw.js`:

**Before (problematic):**
```
coi-serviceworker.js  â† Registered first, added headers
sw.js                 â† PWA caching, could conflict
â†’ Two SWs competing for same scope caused reload loops
```

**After (fixed):**
```
sw.js (v4)            â† Single SW handles both:
                         - COOP/COEP header injection
                         - PWA caching & offline support
```

**Key changes:**
- `addCOIHeaders()` function injects isolation headers
- `networkFirst()` and `cacheFirst()` wrap responses with headers
- Removed `coi-serviceworker.js` entirely
- Fixed `controllerchange` listener to prevent reload loops

### Files Modified

| File | Changes |
|------|---------|
| `js/orchestrator.js` | Enhanced thinking UI, progress callback integration |
| `js/rlm/index.js` | `setProgressCallback()`, `_emitProgress()` methods |
| `css/styles.css` | New thinking indicator styles with colors and animations |
| `sw.js` | COI header injection, version bump to 4 |
| `orchestrator.html` | Removed COI script, fixed reload logic |
| `index.html` | Fixed reload logic |

---

## Phase 3: Optimization & Caching âœ… COMPLETE

### What Was Implemented

#### Phase 3.1: Query Result Caching

**File: `js/rlm/query-cache.js`** (New)

1. **LRU Cache** - Least Recently Used eviction policy with configurable max entries (default: 50)
2. **TTL-based Expiration** - Time-to-live for cache entries (default: 5 minutes)
3. **Smart Cache Keys** - Generated from normalized query + active agent IDs + processing mode
4. **Optional Fuzzy Matching** - Levenshtein distance-based similarity matching for near-duplicate queries
5. **Cache Statistics** - Hit rate, evictions, expirations tracking

**Integration in `js/rlm/index.js`:**
- Cache checked before processing in `process()` and `processWithREPL()`
- Results stored in cache after successful processing
- Auto-invalidation when agents are added/removed/toggled
- `clearCache()` and `getCacheStats()` methods for management

**UI Integration in `orchestrator.html`:**
- "Clear Cache" button added to Knowledge Base header
- Visual feedback on cache clear

#### Phase 3.2: Real-time sub_lm Progress

**Enhanced `js/rlm/repl-environment.js`:**
- `onSubLmStart` callback - Emits when sub_lm call begins with query and depth
- `onSubLmComplete` callback - Emits when call completes with duration and success status

**Enhanced `js/orchestrator.js`:**
- `addThinkingStep()` now supports depth-based indentation
- Displays depth level badges (L1, L2, L3) for recursive calls
- Shows timing badges for completed steps

**Enhanced `css/styles.css`:**
- Depth-based indentation classes (`.depth-1`, `.depth-2`, `.depth-3`)
- Depth badge styling with pink/purple theme
- Timing badge styling with green (success) or orange (warning)

#### Phase 3.3: Token Optimization

**Enhanced `js/rlm/context-store.js`:**

1. **`getCompactContext()`** - Summary-only format for reduced token usage
2. **`getRelevantCompactContext(query)`** - Relevance-filtered context with score-based selection
3. **`getOptimizedREPLContext()`** - Token-optimized context for REPL with transcript limits
4. **`getContextWithBudget(tokenBudget)`** - Automatic context level selection based on token budget
5. **`estimateTokens(text)`** - Rough token estimation for budget management
6. **Helper methods**: `_truncateText()`, `_countBulletPoints()`

### Files Created/Modified

| File | Type | Description |
|------|------|-------------|
| `js/rlm/query-cache.js` | New | LRU cache with TTL and fuzzy matching |
| `js/rlm/index.js` | Modified | Cache integration, new config options |
| `js/rlm/repl-environment.js` | Modified | onSubLmStart/onSubLmComplete callbacks |
| `js/rlm/context-store.js` | Modified | Token optimization methods |
| `js/orchestrator.js` | Modified | Depth-aware thinking steps, clear cache button |
| `css/styles.css` | Modified | Depth indentation, badges, timing styles |
| `orchestrator.html` | Modified | Clear Cache button |

### Cache Configuration

```javascript
// js/rlm/index.js - Cache settings in RLM_CONFIG
{
    enableCache: true,         // Enable query result caching
    cacheMaxEntries: 50,       // Maximum cache entries
    cacheTTL: 5 * 60 * 1000,   // Cache TTL (5 minutes)
    enableFuzzyCache: false    // Enable fuzzy matching for similar queries
}
```

### Cache API

```javascript
const pipeline = getRLMPipeline();

// Clear cache manually
pipeline.clearCache();

// Get cache statistics
const cacheStats = pipeline.getCacheStats();
// Returns: { enabled, hits, misses, evictions, size, hitRate }
```

---

## Configuration Reference

```javascript
// js/rlm/index.js - RLM_CONFIG

{
    // Decomposition
    maxSubQueries: 5,        // Max sub-queries per decomposition
    minRelevanceScore: 2,    // Min score to include agent
    
    // Execution
    maxConcurrent: 3,        // Parallel execution limit
    maxDepth: 3,             // Max recursion depth for sub_lm calls
    tokensPerSubQuery: 800,  // Token budget per sub-query
    timeout: 30000,          // Execution timeout (ms)
    
    // Aggregation
    maxFinalLength: 4000,    // Max response length
    enableLLMSynthesis: true, // Use LLM to synthesize
    deduplicationThreshold: 0.7, // Similarity threshold
    
    // REPL
    enableREPL: true,        // Master REPL switch
    replTimeout: 30000,      // REPL execution timeout
    autoInitREPL: false,     // Lazy vs eager init
    preferREPL: false,       // REPL over decomposition
    subLmTimeout: 60000,     // Timeout for sub_lm calls
    
    // Feature Flags
    enableRLM: true,         // Master RLM switch
    fallbackToLegacy: true,  // Fallback on error
    enableSyncSubLm: true,   // Enable synchronous sub_lm
    
    // Cache Settings (Phase 3.1)
    enableCache: true,       // Enable query result caching
    cacheMaxEntries: 50,     // Maximum cache entries
    cacheTTL: 300000,        // Cache TTL (5 minutes)
    enableFuzzyCache: false  // Enable fuzzy matching
}
```

---

## API Reference

### RLMPipeline

```javascript
const pipeline = getRLMPipeline(config);

// Load agents
pipeline.loadAgents(agentsArray);

// Process query
const result = await pipeline.process(query, llmCall, context);

// Check REPL status
const ready = pipeline.isREPLReady();
await pipeline.initializeREPL();

// Get statistics
const stats = pipeline.getStats();

// Reset
pipeline.reset();
```

### ContextStore

```javascript
const store = getContextStore();

store.loadAgents(agents);
store.getActiveAgents();
store.queryAgents(query);       // Returns relevance-scored agents
store.toPythonDict();           // Export for REPL

// Phase 3.3: Token optimization methods
store.getCompactContext();                  // Summary-only format
store.getRelevantCompactContext(query);     // Relevance-filtered
store.getOptimizedREPLContext();            // Token-limited for REPL
store.getContextWithBudget(4000);           // Auto-select detail level
store.estimateTokens(text);                 // Rough token count
```

### REPLEnvironment

```javascript
const repl = getREPLEnvironment(options);

// Set LLM callback for sub_lm calls (Phase 2.2)
repl.setLLMCallback(async (query, context) => {
    return await callLLM(query, context);
});

await repl.initialize();
const result = await repl.execute(pythonCode);
await repl.setContext(agentsData);

// Status checks
repl.isReady();
repl.isSyncEnabled();       // SharedArrayBuffer available?
repl.getCapabilities();     // Full capability info
repl.getSubLmStats();       // sub_lm call statistics

repl.terminate();
```

---

## Metrics & Monitoring

The RLM pipeline tracks these metrics:

```javascript
pipeline.getStats();
// Returns:
{
    queriesProcessed: 42,
    totalSubQueries: 156,
    avgExecutionTime: 2340,  // ms
    strategies: {
        direct: 12,
        parallel: 18,
        'map-reduce': 8,
        iterative: 4
    },
    replExecutions: 15,
    replErrors: 2,
    subLmCalls: 28,        // Phase 2.2
    subLmErrors: 1,
    cacheHits: 15,         // Phase 3.1
    cacheMisses: 27,
    repl: {
        isReady: true,
        syncEnabled: true,
        subLm: {
            totalCalls: 28,
            successfulCalls: 27,
            failedCalls: 1,
            avgTime: 1250
        },
        capabilities: {
            syncEnabled: true,
            sharedArrayBufferSupported: true,
            maxRecursionDepth: 3
        }
    },
    cache: {               // Phase 3.1
        enabled: true,
        size: 12,
        maxSize: 50,
        hits: 15,
        misses: 27,
        hitRate: '35.7%'
    }
}
```

---

## Known Limitations

1. **Pyodide Size**: ~10MB initial download (cached after first load)
2. **No NumPy/Pandas**: Only standard library available in REPL
3. **SharedArrayBuffer Requirements**: Requires HTTPS and COOP/COEP headers for sync sub_lm
4. **Context Size**: Large agent data may slow REPL operations
5. **No Persistence**: REPL state resets on page reload
6. **Recursion Depth**: Limited to 3 levels to prevent runaway LLM calls

---

## Next Steps

1. ~~**Immediate**: Add result caching for repeated queries~~ âœ… Phase 3.1
2. ~~**Short-term**: Implement progress indicators during sub_lm execution~~ âœ… Phase 3.2
3. ~~**Medium-term**: Token optimization~~ âœ… Phase 3.3
4. **Future**: Sub-query batching for similar questions
5. **Long-term**: Explore fine-tuning for domain-specific code generation
6. **Long-term**: Persistent cache using IndexedDB

---

## Hybrid Focus Milestones 4â€“6 (Guardrails, Instrumentation, Rollout)

### Milestone 4 â€” Guardrails + Token Budgeting âœ…
- Preflight token estimation and prompt budgeting for LLM calls.
- Auto-reduction of shadow retrieval slices when budget is exceeded.
- Recursion caps wired to REPL max depth and sub_lm limits.
- SWM fallback context when a prompt overflows budget.

### Milestone 5 â€” Instrumentation + Memory Debug Panel âœ…
- Token breakdowns and retrieval diagnostics attached to prompt logs.
- Memory Debug Panel shows state block, working window, focus summaries, and shadow retrieval slices.
- Guardrail telemetry surfaced alongside prompt metrics.

### Milestone 6 â€” Evaluation + Rollout âœ…
- Feature flags exposed in UI for shadow prompts, focus tracking, and guardrails.
- QA + benchmark checklist added for rollout validation.

#### QA Steps (Run Before Rollout)
1. Enable **Shadow Prompt** and **Prompt Guardrails** toggles.
2. Run a test prompt; confirm token estimates + retrieval stats appear in metrics.
3. Force high-context prompts to confirm retrieval trimming + SWM fallback behavior.
4. Trigger a focus completion (budget pressure or tool call threshold).
5. Validate Memory Debug Panel shows state block + focus summaries.

#### A/B Benchmark Checklist
- **Cohort split:** 90/10 or 80/20 for feature-flagged users.
- **Metrics:** p50/p95 latency, average tokens per response, cache hit rate, and user satisfaction.
- **Quality:** manual rubric scoring on representative queries.
- **Safety:** ensure no prompt overflows, track guardrail trim rates.
- **Decision:** expand cohort only if latency/cost improve with neutral or better quality scores.

---

## References

- [RLM Paper](https://arxiv.org/abs/2512.24601) - Zhang, Kraska & Khattab
- [Pyodide Documentation](https://pyodide.org/en/stable/)
- [Web Workers API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API)
